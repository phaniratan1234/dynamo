# DYNAMO Configuration - Cloud Version (Optimized for T4 GPUs)
# 2 EPOCHS FOR QUICK TRAINING!

# Model architecture settings
model:
  model_name: "roberta-base"
  hidden_size: 768
  num_attention_heads: 12
  num_hidden_layers: 12
  intermediate_size: 3072
  
# Task specifications
tasks:
  sentiment:
    task_type: "classification"
    num_labels: 2
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.1
    
  qa:
    task_type: "span_prediction"
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.1
    
  summarization:
    task_type: "generation"
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.1
    
  code_generation:
    task_type: "generation"
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.1
    
  translation:
    task_type: "generation"
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.1

# Router settings
router:
  hidden_size: 512
  num_experts: 5
  temperature: 1.0
  gumbel_hard: false
  
# Data settings (OPTIMIZED for cloud)
data:
  max_length: 256
  train_batch_size: 16  # Optimized for T4 GPU
  eval_batch_size: 32
  num_workers: 2
  
  # Full datasets for training (cloud can handle more)
  max_train_samples: 10000  # Moderate size for cloud training
  max_eval_samples: 1000
  max_test_samples: 1000

# Training settings (2 EPOCHS FOR QUICK TRAINING!)
training:
  # Phase 1: Individual LoRA training
  phase1:
    num_epochs: 2  # Quick training - 2 epochs per adapter
    learning_rate: 5e-4
    weight_decay: 1e-2
    warmup_ratio: 0.1
    gradient_accumulation_steps: 2  # Effective batch size = 32
    
  # Phase 2: Router training  
  phase2:
    num_epochs: 2  # Quick router training
    learning_rate: 1e-3
    weight_decay: 1e-3
    warmup_ratio: 0.1
    gradient_accumulation_steps: 2
    
  # Phase 3: Joint fine-tuning
  phase3:
    num_epochs: 2  # Quick joint fine-tuning
    learning_rate: 2e-4
    weight_decay: 1e-3
    warmup_ratio: 0.1
    gradient_accumulation_steps: 2

# Loss function weights
loss_weights:
  task_loss: 1.0
  load_balance_loss: 0.1
  efficiency_loss: 0.05
  consistency_loss: 0.01
  entropy_regularization: 0.01

# Evaluation settings
evaluation:
  eval_batch_size: 32
  logging_steps: 50
  eval_steps: 200
  save_steps: 500
  eval_after_each_phase: true

# System settings
device: "cuda"
seed: 42
checkpoint_dir: "./checkpoints"
log_dir: "./logs"
cache_dir: "./cache"

# Weights & Biases settings (DISABLED for Kaggle)
use_wandb: false
experiment_name: "dynamo_cloud_2epochs"
